{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import swifter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "import tensorflow_text\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D, GRU, Bidirectional\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/User/Desktop/FINAL-THESIS/training_set_en_dataset_without_aug.csv')\n",
    "df_val = pd.read_csv('C:/Users/User/Desktop/FINAL-THESIS/test_set_en_dataset_without_aug.csv')\n",
    "df_test = pd.read_csv('C:/Users/User/Desktop/FINAL-THESIS/val_set_en_dataset_without_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labels = [\"abusive\", \"hateful\", \"offensive\", \"disrespectful\", \"fearful\", \"normal\"]\n",
    "n_labels = len(output_labels)\n",
    "\n",
    "# Discard Empty Tweets if There Any.\n",
    "def discard_empty_tweets(df):\n",
    "    empty_tweet_filter = df[\"tweet\"] != \"\"\n",
    "    df = df[empty_tweet_filter]\n",
    "    df = df.dropna()\n",
    "\n",
    "\n",
    "discard_empty_tweets(df_train)\n",
    "discard_empty_tweets(df_val)\n",
    "discard_empty_tweets(df_test)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to int type.\n",
    "\n",
    "def transform_to_int(df):\n",
    "    df['abusive'] = df['abusive'].astype(np.int)\n",
    "    df['hateful'] = df['hateful'].astype(np.int)\n",
    "    df['offensive'] = df['offensive'].astype(np.int)\n",
    "    df['disrespectful'] = df['disrespectful'].astype(np.int)\n",
    "    df['fearful'] = df['fearful'].astype(np.int)\n",
    "    df['normal'] = df['normal'].astype(np.int)\n",
    "\n",
    "transform_to_int(df_train)\n",
    "transform_to_int(df_val)\n",
    "transform_to_int(df_test)\n",
    "\n",
    "df_train['one_hot_labels'] = list(df_train[output_labels].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Distribution of Target Labels in the Dataset\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "toxic_comments_labels.sum(axis=0).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying indices of 'one_hot_labels' entries that only occur once\n",
    "df_train = df_train.reset_index()\n",
    "label_counts = df_train.one_hot_labels.astype(str).value_counts()\n",
    "one_freq = label_counts[label_counts==1].keys()\n",
    "one_freq_idxs = sorted(list(df_train[df_train.one_hot_labels.astype(str).isin(one_freq)].index), reverse=True)\n",
    "print('df label indices with only one instance: ', one_freq_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = df_train[\"tweet_cleaned\"].values\n",
    "test_sentences = df_test[\"tweet_cleaned\"].values\n",
    "val_sentences = df_val[\"tweet_cleaned\"].values\n",
    "\n",
    "train_y = df_train[output_labels].values\n",
    "test_y = df_test[output_labels].values\n",
    "val_y = df_val[output_labels].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n",
    "preprocessor = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "encoder_inputs = preprocessor(text_input)\n",
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\", \n",
    "    trainable=True)\n",
    "outputs = encoder(encoder_inputs)\n",
    "pooled_output = outputs[\"pooled_output\"]      # [batch_size, 1024].\n",
    "sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 1024].\n",
    "\n",
    "x = GlobalAveragePooling1D()(sequence_output)\n",
    "x = Dropout(0.3)(x)\n",
    "output = Dense(n_labels, activation='sigmoid', name='outputs')(x)\n",
    "\n",
    "model = Model(inputs=text_input, outputs=output)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-4, decay=1e-6), \n",
    "              metrics=['accuracy', \n",
    "                       tf.keras.metrics.Precision(name='precision'),\n",
    "                       tf.keras.metrics.Recall(name='recall'),\n",
    "                       tf.keras.metrics.AUC(name='auc',multi_label=True, thresholds=[0, 0.5]),\n",
    "                       tf.keras.metrics.TruePositives(name='TP'),\n",
    "                       tf.keras.metrics.TrueNegatives(name='TN'),\n",
    "                       tf.keras.metrics.FalsePositives(name='FP'),\n",
    "                       tf.keras.metrics.FalseNegatives(name='FN')])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath=\"../../FINAL-THESIS/BERT_weights_model_final_without_aug.hdf5\", \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train=train_sentences, y_train=train_y, epochs=4, \n",
    "                    batch_size=32, callbacks=[checkpointer],\n",
    "                    validation_data=(val_sentences, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Metrics Plots\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['precision'])\n",
    "plt.plot(history.history['val_precision'])\n",
    "plt.title('model precision')\n",
    "plt.ylabel('precision')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auc'])\n",
    "plt.plot(history.history['val_auc'])\n",
    "plt.title('model auc')\n",
    "plt.ylabel('auc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(filepath='../../NN_weights.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(train_sentences, train_y, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing Metrics\n",
    "F1_score = 2 * (results[2] * results[3])/(results[2] + results[3])\n",
    "AUC=results[4]\n",
    "TP=results[5]\n",
    "TN=results[6]\n",
    "FP=results[7]\n",
    "FN=results[8]\n",
    "\n",
    "\n",
    "#Printing Metrics Results\n",
    "print(\"F1 SCORE: \", F1_score)\n",
    "print(\"AUC SCORE: \", AUC)\n",
    "print(\"True Positives: \", TP)\n",
    "print(\"True Negatives: \", TN)\n",
    "print(\"False Positives: \", FP)\n",
    "print(\"False Negatives: \", FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(train_sentences)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "y_true = train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=output_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
