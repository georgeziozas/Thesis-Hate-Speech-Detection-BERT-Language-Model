{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import codecs\n",
    "import unidecode\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps in tweet cleaning:\n",
    "* Decoding: unicode_escape for extra “\\” before unicode character, then unidecode\n",
    "* Apostrophe handled: there are two characters people use for contraction. “’”(apostrophe) and “‘“(single quote). If these two symbols are both used for contraction, it will be difficult to detect and properly map the right expanded form. So any “’”(apostrophe) is changed to “‘“(single quote)\n",
    "* Contraction check: check if there’s any contracted form, and replace it with its original form\n",
    "* Parsing: done with Spacy\n",
    "* Filtering punctuation, white space, numbers, URL using Spacy methods while keeping the text content of hashtag intact\n",
    "* Removed @mention\n",
    "* Special character removal\n",
    "* Single syllable token removal\n",
    "* Spell correction: it is a simple spell correction dealing with repeated characters such as “sooooo goooood”. If the same character is repeated more than two times, it shortens the repetition to two. For example “sooooo goooood” will be transformed as “soo good”. This is not a perfect solution since even after correction, in case of “soo”, it is not a correct spelling. But at least it will help to reduce feature space by making “sooo”, “soooo”, “sooooo” to the same word “soo”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"cant\": \"cannot\",\n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                       \"here's\": \"here is\",\n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def spacy_cleaner(text):\n",
    "    try:\n",
    "        decoded = unidecode.unidecode(codecs.decode(text, 'unicode_escape'))\n",
    "    except:\n",
    "        decoded = unidecode.unidecode(text)\n",
    "    apostrophe_handled = re.sub(\"’\", \"'\", decoded)\n",
    "    expanded = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in apostrophe_handled.split(\" \")])\n",
    "    parsed = nlp(expanded)\n",
    "    final_tokens = []\n",
    "    for t in parsed:\n",
    "        if t.is_punct or t.is_space or t.like_num or t.like_url or str(t).startswith('@'):\n",
    "            pass\n",
    "        else:\n",
    "            sc_removed = re.sub(\"[^a-zA-Z]\", '', str(t))\n",
    "            if len(sc_removed) > 1:\n",
    "                final_tokens.append(sc_removed)\n",
    "    joined = ' '.join(final_tokens)\n",
    "    spell_corrected = re.sub(r'(.)\\1+', r'\\1\\1', joined)\n",
    "    return spell_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/User/Desktop/FINAL-THESIS/en_dataset.csv')\n",
    "print(\"Shape of dataframe: \", df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating out each of the labels to different words\n",
    "def split_sentiment_outputs(output_label, sentiment_col=\"sentiment\"):\n",
    "    df[output_label] = df[sentiment_col].str.split('_')\n",
    "    \n",
    "\n",
    "# Converting the dataframe to have one column for each of the labels.\n",
    "def transform_data_for_multilabel(output_label):\n",
    "    row = df[output_label]\n",
    "    for index, row in row.items():\n",
    "        z = 0\n",
    "        while z < len(row):\n",
    "            if row[z] == output_label:\n",
    "                df.at[index, output_label] = 1\n",
    "                break\n",
    "            else:\n",
    "                df.at[index, output_label] = 0\n",
    "            z = z + 1\n",
    "            \n",
    "output_labels = [\"abusive\", \"hateful\", \"offensive\", \"disrespectful\", \"fearful\", \"normal\"]\n",
    "n_labels = len(output_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_labels):\n",
    "    split_sentiment_outputs(output_labels[i])\n",
    "    \n",
    "for i in range(n_labels):\n",
    "    transform_data_for_multilabel(output_labels[i])\n",
    "\n",
    "# Removing unnecessary columns from dataframe\n",
    "df = df.drop(['directness', 'annotator_sentiment', 'target', 'group','HITId'], axis=1)\n",
    "\n",
    "# Removing the 'sentiment' column as it is not needed anymore after all necessary information has been extracted\n",
    "df = df.drop(['sentiment'], axis=1)\n",
    "\n",
    "# Removing all duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Applying the tweet cleaning function to each of the tweets\n",
    "df['tweet_cleaned'] = df['tweet'].apply(lambda x: spacy_cleaner(x))\n",
    "\n",
    "# Generating the number of words of each tweet in a column\n",
    "df['tweet_length'] = df['tweet_cleaned'].apply(lambda x: len(x.split(' ')))\n",
    "\n",
    "# Discard Empty Tweets if There Any.\n",
    "empty_tweet_filter = df['tweet_cleaned'] != \"\"\n",
    "df = df[empty_tweet_filter]\n",
    "df = df.dropna()\n",
    "\n",
    "#df = df.drop(columns=['tweet'])\n",
    "\n",
    "print(\"Shape of dataframe: \", df.shape)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Value Counts of Categories of the whole dataset:\\n\")\n",
    "for category in df.columns[1:7]:\n",
    "    print(f\"{category}: {df[category].sum()}\")\n",
    "    \n",
    "\n",
    "#Plot the Distribution of Target Labels in the Dataset\n",
    "df2 = df[df.columns[1:7]]\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "df2.sum(axis=0).plot.bar()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data splitting - TRAIN-TEST-DEV-> 8:1:1\n",
    "def split_df(df):\n",
    "    if len(df) % 2 != 0:  # Handling `df` with `odd` number of rows\n",
    "        df = df.iloc[:-1, :]\n",
    "    df1, df2 =  np.array_split(df, 2)\n",
    "    return df1, df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=2021)\n",
    "df_test, df_val = split_df(df_test)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "print(\"\\n Value Counts of Training set Categories before augmentation and after split:\\n\")\n",
    "for category in df_train.columns[1:7]:\n",
    "    print(f\"{category}: {df_train[category].sum()}\")\n",
    "\n",
    "print(\"\\n Value Counts of Val set Categories before augmentation and after split:\\n\")\n",
    "for category in df_val.columns[1:7]:\n",
    "    print(f\"{category}: {df_val[category].sum()}\")\n",
    "\n",
    "print(\"\\n Value Counts of Test set Categories before augmentation and after split:\\n\")\n",
    "for category in df_test.columns[1:7]:\n",
    "    print(f\"{category}: {df_test[category].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is imbalanced as there are less text examples for some of the labels (eg. fearful, abusive, disrespectful), we will try and increase the number of examples for each of these labels by a text augmentation technique, backtranslation. \n",
    "\n",
    "The back-translation process works in the following way:\n",
    "* Take some sentence and translate to another language\n",
    "* Translate the output sentence back to original language\n",
    "* Check if the new sentence is different from the original sentence. If it is, then we use this new sentence as an augmented version of the original text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")\n",
    "\n",
    "sub_word_with_bert_aug = naw.ContextualWordEmbsAug(\n",
    "    model_path ='bert-base-uncased', action=\"substitute\"\n",
    ")\n",
    "\n",
    "def generate_augmented_text_back_translate(text):\n",
    "    try:\n",
    "        augmented_text = back_translation_aug.augment(text)\n",
    "    except:\n",
    "        augmented_text = text\n",
    "    return augmented_text\n",
    "\n",
    "\n",
    "def generate_augmented_text_sub_words_with_bert(text):\n",
    "    try:\n",
    "        augmented_text = sub_word_with_bert_aug.augment(text)\n",
    "    except:\n",
    "        augmented_text = text\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Back-Translation Augmentation Example:')\n",
    "text = \"Its a pretty horrible time to be a sportsperson at the moment.\"\n",
    "augmented_text = generate_augmented_text_back_translate(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print()\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)\n",
    "\n",
    "\n",
    "print('Subtitute words via BERT embeddings Augmentation Example:')\n",
    "text = \"Its a pretty horrible time to be a sportsperson at the moment.\"\n",
    "augmented_text = generate_augmented_text_sub_words_with_bert(text)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print()\n",
    "print(\"Augmented Text:\")\n",
    "print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to augment examples for a label with both augmenting techniques. Keep in mind that we are only augmenting those tweets which have more than 5 words.\n",
    "# Augmenting tweets with 2 or 3 words doesn't produce good augmentation results.\n",
    "def augment_category_with_back_translate(category_label: str, df):\n",
    "    temp_df1 = pd.DataFrame(columns=df.columns)\n",
    "    df = df[(df[category_label] == 1) & (df['offensive'] == 0)]\n",
    "    indexes = df[df['tweet_length'] > 5].index\n",
    "\n",
    "    for idx in tqdm(indexes):\n",
    "        text = df.loc[idx]['tweet_cleaned']\n",
    "        temp_df1 = temp_df1.append(df.loc[[idx]].assign(**{'tweet_cleaned': generate_augmented_text_back_translate(text)}), \n",
    "                  ignore_index=True)\n",
    "    return temp_df1\n",
    "\n",
    "\n",
    "def augment_category_with_bert_embeddings_subs(category_label: str, df):\n",
    "    temp_df2 = pd.DataFrame(columns=df.columns)\n",
    "    df = df[(df[category_label] == 1) & (df['offensive'] == 0)]\n",
    "    indexes = df[df['tweet_length'] > 5].index\n",
    "\n",
    "    for idx in tqdm(indexes):\n",
    "        text = df.loc[idx]['tweet_cleaned']\n",
    "        temp_df2 = temp_df2.append(df.loc[[idx]].assign(**{'tweet_cleaned': generate_augmented_text_sub_words_with_bert(text)}), \n",
    "                  ignore_index=True)\n",
    "    return temp_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in ['fearful', 'abusive', 'disrespectful']:\n",
    "    temp_df1 = augment_category_with_back_translate(label, df_train)\n",
    "    temp_df2 = augment_category_with_bert_embeddings_subs(label, df_train)\n",
    "    df_train = pd.concat([df_train, temp_df1], ignore_index = True)\n",
    "    df_train = pd.concat([df_train, temp_df2], ignore_index = True)\n",
    "    df_train = df_train.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of Training Set After Augmentation: \", df_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-89005ea0eb4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n Value Counts of Training set Categories AFTER augmentation and after split:\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)\n",
    "print(df_val.shape)\n",
    "\n",
    "print(\"\\n Value Counts of Training set Categories AFTER augmentation and after split:\\n\")\n",
    "for category in df_train.columns[1:7]:\n",
    "    print(f\"{category}: {df_train[category].sum()}\")\n",
    "\n",
    "print(\"\\n Value Counts of Val set Categories AFTER augmentation and after split:\\n\")\n",
    "for category in df_val.columns[1:7]:\n",
    "    print(f\"{category}: {df_val[category].sum()}\")\n",
    "\n",
    "print(\"\\n Value Counts of Test set Categories AFTER augmentation and after split:\\n\")\n",
    "for category in df_test.columns[1:7]:\n",
    "    print(f\"{category}: {df_test[category].sum()}\")\n",
    "\n",
    "\n",
    "    \n",
    "#Plot the Distribution of Target Labels in the Dataset\n",
    "df3 = df_train[df_train.columns[1:7]]\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "df3.sum(axis=0).plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of examples for labels of 'fearful', 'abusive' and 'disrespectful' have increased and the whole data is somewhat more balanced now for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns=['tweet_length'])\n",
    "df_test = df_test.drop(columns=['tweet_length'])\n",
    "df_val = df_val.drop(columns=['tweet_length'])\n",
    "\n",
    "df_train.to_csv('C:/Users/User/Desktop/FINAL-THESIS/training_set_en_dataset_aug.csv', index=False)\n",
    "df_test.to_csv('C:/Users/User/Desktop/FINAL-THESIS/test_set_en_dataset.csv', index=False)\n",
    "df_val.to_csv('C:/Users/User/Desktop/FINAL-THESIS/val_set_en_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
